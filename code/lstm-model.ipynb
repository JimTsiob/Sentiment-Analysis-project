{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":8153677,"sourceType":"datasetVersion","datasetId":4822755}],"dockerImageVersionId":30702,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"## Imports and downloads","metadata":{}},{"cell_type":"code","source":"!pip install pandarallel\n!pip install torch\n!pip install gensim\n\nprint(\"\\n\")\nprint(\"INSTALLATIONS COMPLETE.\")","metadata":{"execution":{"iopub.status.busy":"2024-04-20T17:35:51.611044Z","iopub.execute_input":"2024-04-20T17:35:51.611512Z","iopub.status.idle":"2024-04-20T17:36:41.700858Z","shell.execute_reply.started":"2024-04-20T17:35:51.611469Z","shell.execute_reply":"2024-04-20T17:36:41.699259Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.feature_extraction.text import HashingVectorizer\n\nfrom sklearn.metrics import classification_report, accuracy_score, f1_score\n\nimport nltk\nimport seaborn as sns\nfrom nltk.corpus import stopwords\nfrom sklearn.svm import LinearSVC, SVC\nfrom pandarallel import pandarallel\nfrom nltk.tokenize import word_tokenize\nfrom nltk.stem import WordNetLemmatizer\nnltk.download('stopwords')\nnltk.download('punkt')\nfrom gensim.models import Word2Vec\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport matplotlib.pyplot as plt\nimport re\n\n\nimport subprocess\n\n# Download and unzip wordnet\ntry:\n    nltk.data.find('wordnet.zip')\nexcept:\n    nltk.download('wordnet', download_dir='/kaggle/working/')\n    command = \"unzip /kaggle/working/corpora/wordnet.zip -d /kaggle/working/corpora\"\n    subprocess.run(command.split())\n    nltk.data.path.append('/kaggle/working/')\n    \n\nprint('Imports done.')","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-04-20T19:19:17.070264Z","iopub.execute_input":"2024-04-20T19:19:17.070800Z","iopub.status.idle":"2024-04-20T19:19:17.120350Z","shell.execute_reply.started":"2024-04-20T19:19:17.070761Z","shell.execute_reply":"2024-04-20T19:19:17.116750Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"electronics_dataset = pd.read_csv('../input/amazon-reviews-2018-electronics/labeled_electronics_dataset.csv')\n\nelectronics_dataset.head()","metadata":{"execution":{"iopub.status.busy":"2024-04-20T17:49:54.046338Z","iopub.execute_input":"2024-04-20T17:49:54.047303Z","iopub.status.idle":"2024-04-20T17:49:54.311620Z","shell.execute_reply.started":"2024-04-20T17:49:54.047261Z","shell.execute_reply":"2024-04-20T17:49:54.310167Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Data pre-processing","metadata":{}},{"cell_type":"code","source":"# Check for NaN values\nprint(\"NaN (before cleanup) ?: \\n\", electronics_dataset.isnull().sum())\n\nelectronics_dataset['reviewText'] = electronics_dataset['reviewText'].fillna('')\n\nprint(\"NaN (after cleanup) ?: \\n\", electronics_dataset.isnull().sum())","metadata":{"execution":{"iopub.status.busy":"2024-04-20T17:50:36.811666Z","iopub.execute_input":"2024-04-20T17:50:36.812236Z","iopub.status.idle":"2024-04-20T17:50:36.857753Z","shell.execute_reply.started":"2024-04-20T17:50:36.812193Z","shell.execute_reply":"2024-04-20T17:50:36.856297Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Text preprocessing for reviewText column\n# Lower all text\n\nelectronics_dataset['reviewText'] = electronics_dataset['reviewText'].str.lower()\n\n# Initialize pandarallel\n# I used pandarallel because it applies the functions much faster than a normal pandas apply.\npandarallel.initialize(nb_workers=4,progress_bar=True)\n\n# Remove all special characters\ndef remove_special_chars(text):\n    return ''.join(x if x.isalnum() else ' ' for x in text)\n\nelectronics_dataset['reviewText'] = electronics_dataset['reviewText'].parallel_apply(remove_special_chars)\n\n# get stopwords.\nstop_words = set(stopwords.words('english'))\n\n# Remove stop_words\ndef remove_stopwords(text):\n    words = word_tokenize(text)\n    return [x for x in words if x not in stop_words]\n\n\nelectronics_dataset['reviewText'] = electronics_dataset['reviewText'].parallel_apply(remove_stopwords)\n\n# Lemmatization\ndef lemmatize_word(text):\n    wordnet = WordNetLemmatizer()\n    return \" \".join([wordnet.lemmatize(word) for word in text])\n\nelectronics_dataset['reviewText'] = electronics_dataset['reviewText'].parallel_apply(lemmatize_word)\n\ndef remove_numbers(text):\n    return re.sub(r'\\d+', '', text)\n\nelectronics_dataset['reviewText'] = electronics_dataset['reviewText'].parallel_apply(remove_numbers)\n\nprint('Example of preprocessing train: ')\nprint(electronics_dataset['reviewText'][0])","metadata":{"execution":{"iopub.status.busy":"2024-04-20T17:52:31.470426Z","iopub.execute_input":"2024-04-20T17:52:31.470940Z","iopub.status.idle":"2024-04-20T17:52:42.260693Z","shell.execute_reply.started":"2024-04-20T17:52:31.470891Z","shell.execute_reply":"2024-04-20T17:52:42.259070Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Add our data to Torch Tensors so they can be later used in the LSTM neural network.\n\ndef simple_tokenizer(sentence):\n    tokens = sentence.split()  # Split the sentence into a list of words\n    return tokens\n\n# tokenize reviewText column and perform below processes for our dataset.\nelectronics_dataset['Tokens'] = electronics_dataset['reviewText'].apply(simple_tokenizer)\n\nX = electronics_dataset['Tokens']\ny = electronics_dataset['Label']\n\nX_train_tokenized, X_test_tokenized, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\n# Train Word2Vec model\n\nword2vec_model = Word2Vec(X_train_tokenized, vector_size=100, window=5, min_count=1, workers=4)\n\n# Creates embeddings for each review\ndef get_review_embedding(review):\n    word_embeddings = [word2vec_model.wv[word] for word in review if word in word2vec_model.wv]\n    if not word_embeddings:\n        zeroes_list = [0] * 100\n        return zeroes_list\n            \n#         return None  # Handle the case where no words have embeddings\n    review_embedding = sum(word_embeddings) / len(word_embeddings)  # Simple average\n    return review_embedding\n\n# Convert review tokens to embeddings\nreview_embeddings_train = [get_review_embedding(review) for review in X_train_tokenized]\nreview_embeddings_test = [get_review_embedding(review) for review in X_test_tokenized]\n\n# Convert labels to one-hot encoded tensors\nlabel_embeddings_train = pd.get_dummies(y_train).values\nlabel_embeddings_test = pd.get_dummies(y_test).values\n\n# Below part is used to correlate the numbers 0,1,2 to their respective labels (POSITIVE,NEUTRAL,NEGATIVE) for predictions\nlabel_encoded_train = pd.get_dummies(y_train)\nlabel_encoded_test = pd.get_dummies(y_test)\n\nlabel_mapping_train = {i: label for i, label in enumerate(label_encoded_train.columns)}\nlabel_mapping_test = {i: label for i, label in enumerate(label_encoded_test.columns)}\n\n# Convert to PyTorch tensors\nX_train_tensor = torch.FloatTensor(review_embeddings_train)\ny_train_tensor = torch.FloatTensor(label_embeddings_train)\n\nX_test_tensor = torch.FloatTensor(review_embeddings_test)\ny_test_tensor = torch.FloatTensor(label_embeddings_test)\n\n\nprint('X_train_tensor shape:', X_train_tensor.shape)\nprint('y_train_tensor shape:', y_train_tensor.shape)\n\nprint('X_test_tensor shape: ', X_test_tensor.shape)\nprint('y_test_tensor shape: ', y_test_tensor.shape)","metadata":{"execution":{"iopub.status.busy":"2024-04-20T19:54:52.381349Z","iopub.execute_input":"2024-04-20T19:54:52.383246Z","iopub.status.idle":"2024-04-20T19:54:58.268477Z","shell.execute_reply.started":"2024-04-20T19:54:52.383180Z","shell.execute_reply":"2024-04-20T19:54:58.267304Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_train_tensor = X_train_tensor.unsqueeze(1)  # Adds a dimension at index 1, used for LSTM\nX_test_tensor = X_test_tensor.unsqueeze(1)\n\n#Initialise dataloaders\ntrain_dataset = torch.utils.data.TensorDataset(X_train_tensor, y_train_tensor) #class to represent the data as list of tensors. x=input_features, y=labels\ntest_dataset = torch.utils.data.TensorDataset(X_test_tensor, y_test_tensor)\n\ntrain_dataloader = torch.utils.data.DataLoader(train_dataset, batch_size=64, shuffle=True)\ntest_dataloader = torch.utils.data.DataLoader(test_dataset, batch_size=64)\n\nprint('Data loading complete.')","metadata":{"execution":{"iopub.status.busy":"2024-04-20T19:54:58.270684Z","iopub.execute_input":"2024-04-20T19:54:58.271171Z","iopub.status.idle":"2024-04-20T19:54:58.281023Z","shell.execute_reply.started":"2024-04-20T19:54:58.271130Z","shell.execute_reply":"2024-04-20T19:54:58.279718Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## LSTM Model creation and evaluation","metadata":{}},{"cell_type":"code","source":"# RNN stacked bi-directional model\nclass RNN(nn.Module):\n    def __init__(self, cell_type, input_size, hidden_size, output_size, num_hidden_layers):\n        super(RNN, self).__init__()\n        \n        cells = {\n          \"RNN\" : nn.RNN,\n          \"LSTM\"    : nn.LSTM,\n          \"GRU\"     : nn.GRU\n        }\n\n        self.cell_type = cell_type\n\n        self.rnn = cells[cell_type](         # Pick the specific model\n            input_size = input_size,           # Number of features for each time step\n            hidden_size = hidden_size,         # rnn hidden units\n            batch_first = True, # input & output will have batch size as 1s dimension. e.g. (batch, time_step, input_size)\n            bidirectional = True, # making RNN bidirectional\n            num_layers = num_hidden_layers, # Making RNN stacked with additional layers\n            dropout = 0.2 # Using dropout\n        )\n        \n#         self.lstm = nn.LSTM(input_size, hidden_size, batch_first=True,bidirectional=True,num_layers=2,dropout=0.2)\n        \n        # Adding more hidden layers based on parameter\n        self.hidden_layers = nn.ModuleList()\n        for _ in range(num_hidden_layers):\n            self.hidden_layers.append(nn.Linear(hidden_size * 2, hidden_size * 2))\n        \n        self.fc = nn.Linear(hidden_size * 2, output_size)\n\n    def forward(self, x):\n        if self.cell_type == 'LSTM':\n            r_out, (h_n, h_c) = self.rnn(x)\n        else:\n            r_out, h_n = self.rnn(x)\n#         r_out, _ = self.lstm(x)\n        \n        # Pass through additional hidden layers with ReLU activation\n        for layer in self.hidden_layers:\n            r_out = F.relu(layer(r_out))\n        \n        output = self.fc(r_out[:, -1, :])  # Take the output from the last time step\n        return output","metadata":{"execution":{"iopub.status.busy":"2024-04-20T19:54:58.283036Z","iopub.execute_input":"2024-04-20T19:54:58.283454Z","iopub.status.idle":"2024-04-20T19:54:58.300301Z","shell.execute_reply.started":"2024-04-20T19:54:58.283413Z","shell.execute_reply":"2024-04-20T19:54:58.298867Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Hyperparameters\ninput_size = 100 \nhidden_size = 50  \noutput_size = 3 \nlearning_rate = 0.001\nnum_hidden_layers = 2\n\n# Create the model, loss function, and optimizer\ncell_type = 'LSTM'\n# net = RNN(cell_type, input_size, hidden_size, output_size) # \nmodel = RNN(cell_type, input_size, hidden_size, output_size, num_hidden_layers)\nloss_func = nn.CrossEntropyLoss()\noptimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n\n# Train NN for one epoch so we can get Learning curve for steps.\n\nfor epoch in range(1):\n    batch_losses = []\n    for x_batch, y_batch in train_dataloader:\n        y_pred = model(x_batch)\n\n        loss = loss_func(y_pred, y_batch)\n        batch_losses.append(loss.item())\n        # print('y_pred=', y_pred[0])\n        #Delete previously stored gradients\n        optimizer.zero_grad()\n        #Perform backpropagation starting from the loss calculated in this epoch\n        \n        loss.backward()\n        #Update model's weights based on the gradients calculated during backprop\n        optimizer.step()\n    \n#     print(f\"Epoch {epoch:3}: Loss = {sum(batch_losses)/len(train_dataloader):.5f}\")\n    \n    model.eval()\n    with torch.no_grad():\n        test_batch_losses = []\n        for x_batch, y_batch in test_dataloader:\n            y_pred_test = model(x_batch)\n            # Compute and print/validation loss or other metrics\n            test_loss = loss_func(y_pred_test, y_batch)\n            test_batch_losses.append(test_loss.item())\n    \n    # Check validation loss to make sure we don't get overfitting\n    print(f'Epoch {epoch:3} \\t\\t Training Loss: {sum(batch_losses)/len(train_dataloader):.5f} \\t\\t Validation Loss: {sum(test_batch_losses) / len(test_dataloader):.5f}')\n\n    \n# Plot results (Learning curve)\nplt.figure(figsize=(12,5))\nplt.plot(batch_losses)\nplt.title('Learning Curve')\nplt.xlabel('# of steps', fontsize=12)\nplt.ylabel('CE - Loss', fontsize=12)\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2024-04-20T18:46:06.136265Z","iopub.execute_input":"2024-04-20T18:46:06.136914Z","iopub.status.idle":"2024-04-20T18:46:09.634518Z","shell.execute_reply.started":"2024-04-20T18:46:06.136869Z","shell.execute_reply":"2024-04-20T18:46:09.632951Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# LSTM run\n# Hyperparameters\ninput_size = 100 \nhidden_size = 50  \noutput_size = 3 \nlearning_rate = 0.001\nnum_hidden_layers = 2\nf1_scores = {} # here I will save all f1 scores and later on get the best NN's results\nvalid_preds = {} # here I will save all valid predictions and later on get the best NN's results\ntest_preds = {} # here I will save all test predictions and later on get the best NN's results\n\n# Set random seed for reproducibility\ntorch.manual_seed(42)\nnp.random.seed(42)\n\n# Create the model, loss function, and optimizer\ncell_type = 'LSTM'\n# net = RNN(cell_type, input_size, hidden_size, output_size) # \nmodel = RNN(cell_type,input_size, hidden_size, output_size, num_hidden_layers)\nloss_func = nn.CrossEntropyLoss()\noptimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n\n\nfor epoch in range(50):\n    batch_losses = []\n    for x_batch, y_batch in train_dataloader:\n        y_pred = model(x_batch)\n\n        loss = loss_func(y_pred, y_batch)\n        batch_losses.append(loss.item())\n        # print('y_pred=', y_pred[0])\n        #Delete previously stored gradients\n        optimizer.zero_grad()\n        #Perform backpropagation starting from the loss calculated in this epoch\n        \n        loss.backward()\n        #Update model's weights based on the gradients calculated during backprop\n        optimizer.step()\n    \n    \n    # Check validation loss to make sure we don't get overfitting\n    print(f'Epoch {epoch:3} \\t\\t Training Loss: {sum(batch_losses)/len(train_dataloader):.5f}')\n\n\n# Test set predictions\n# Generate test predictions and evaluate test set\nmodel.eval()\npredictions_test = []\n\n# test results\nwith torch.no_grad():\n    for x_batch,y_batch in test_dataloader:\n        outputs = model(x_batch)\n        _, predicted = torch.max(outputs, 1)\n\n        predictions_test.extend(predicted.tolist())\n    \n# convert 0,1,2 predictions to their original text form (POSITIVE, NEUTRAL , NEGATIVE)\noriginal_label_predictions_test_lstm = [label_mapping_test[pred] for pred in predictions_test]\n\nprint(\"\\n\")\nprint(\"================= LSTM NN SCORES =================\")\nprint(classification_report(y_test,original_label_predictions_test_lstm))\n\nprint(\"accuracy: \",accuracy_score(y_test,original_label_predictions_test_lstm))\nprint(\"f1: \",f1_score(y_test,original_label_predictions_test_lstm,average='micro'))\nprint(\"total f1: \",f1_score(y_test,original_label_predictions_test_lstm,average=None))","metadata":{"execution":{"iopub.status.busy":"2024-04-20T19:56:27.607811Z","iopub.execute_input":"2024-04-20T19:56:27.608279Z","iopub.status.idle":"2024-04-20T19:58:46.823423Z","shell.execute_reply.started":"2024-04-20T19:56:27.608242Z","shell.execute_reply":"2024-04-20T19:58:46.822163Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}