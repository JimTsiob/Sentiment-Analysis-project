{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":8153677,"sourceType":"datasetVersion","datasetId":4822755}],"dockerImageVersionId":30699,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"## Imports and downloads","metadata":{}},{"cell_type":"code","source":"!pip install pandarallel\n!pip install torch\n!pip install transfomers\n\nprint(\"\\n\")\nprint(\"INSTALLATIONS COMPLETE.\")","metadata":{"execution":{"iopub.status.busy":"2024-05-03T11:50:46.931647Z","iopub.execute_input":"2024-05-03T11:50:46.932312Z","iopub.status.idle":"2024-05-03T11:51:16.966226Z","shell.execute_reply.started":"2024-05-03T11:50:46.932279Z","shell.execute_reply":"2024-05-03T11:51:16.965096Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"Collecting pandarallel\n  Downloading pandarallel-1.6.5.tar.gz (14 kB)\n  Preparing metadata (setup.py) ... \u001b[?25ldone\n\u001b[?25hRequirement already satisfied: dill>=0.3.1 in /opt/conda/lib/python3.10/site-packages (from pandarallel) (0.3.8)\nRequirement already satisfied: pandas>=1 in /opt/conda/lib/python3.10/site-packages (from pandarallel) (2.1.4)\nRequirement already satisfied: psutil in /opt/conda/lib/python3.10/site-packages (from pandarallel) (5.9.3)\nRequirement already satisfied: numpy<2,>=1.22.4 in /opt/conda/lib/python3.10/site-packages (from pandas>=1->pandarallel) (1.26.4)\nRequirement already satisfied: python-dateutil>=2.8.2 in /opt/conda/lib/python3.10/site-packages (from pandas>=1->pandarallel) (2.9.0.post0)\nRequirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.10/site-packages (from pandas>=1->pandarallel) (2023.3.post1)\nRequirement already satisfied: tzdata>=2022.1 in /opt/conda/lib/python3.10/site-packages (from pandas>=1->pandarallel) (2023.4)\nRequirement already satisfied: six>=1.5 in /opt/conda/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas>=1->pandarallel) (1.16.0)\nBuilding wheels for collected packages: pandarallel\n  Building wheel for pandarallel (setup.py) ... \u001b[?25ldone\n\u001b[?25h  Created wheel for pandarallel: filename=pandarallel-1.6.5-py3-none-any.whl size=16672 sha256=cc03ea5fe3aee07d929ce718112a8298763803a6a5965d2cefac973dc80ada02\n  Stored in directory: /root/.cache/pip/wheels/50/4f/1e/34e057bb868842209f1623f195b74fd7eda229308a7352d47f\nSuccessfully built pandarallel\nInstalling collected packages: pandarallel\nSuccessfully installed pandarallel-1.6.5\nRequirement already satisfied: torch in /opt/conda/lib/python3.10/site-packages (2.1.2)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from torch) (3.13.1)\nRequirement already satisfied: typing-extensions in /opt/conda/lib/python3.10/site-packages (from torch) (4.9.0)\nRequirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch) (1.12)\nRequirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch) (3.2.1)\nRequirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch) (3.1.2)\nRequirement already satisfied: fsspec in /opt/conda/lib/python3.10/site-packages (from torch) (2024.2.0)\nRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch) (2.1.3)\nRequirement already satisfied: mpmath>=0.19 in /opt/conda/lib/python3.10/site-packages (from sympy->torch) (1.3.0)\n\u001b[31mERROR: Could not find a version that satisfies the requirement transfomers (from versions: none)\u001b[0m\u001b[31m\n\u001b[0m\u001b[31mERROR: No matching distribution found for transfomers\u001b[0m\u001b[31m\n\u001b[0m\n\nINSTALLATIONS COMPLETE.\n","output_type":"stream"}]},{"cell_type":"code","source":"import numpy as np \nimport pandas as pd\nimport numpy as np\nimport pandas as pd\nfrom transformers import DistilBertTokenizer, DistilBertForSequenceClassification\nfrom matplotlib import pyplot as plt\nimport seaborn as sns\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.metrics import confusion_matrix,classification_report,accuracy_score,roc_auc_score,roc_curve,f1_score\nimport time\nfrom pandarallel import pandarallel\nfrom sklearn.model_selection import train_test_split\nimport re\n\n\n# HW2 new imports\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nimport random\n\nimport nltk\nfrom nltk.tokenize import word_tokenize\nfrom nltk.stem import WordNetLemmatizer\nfrom nltk.corpus import stopwords\nnltk.download('stopwords')\nnltk.download('punkt')\n\nimport subprocess\n\n# Download and unzip wordnet\ntry:\n    nltk.data.find('wordnet.zip')\nexcept:\n    nltk.download('wordnet', download_dir='/kaggle/working/')\n    command = \"unzip /kaggle/working/corpora/wordnet.zip -d /kaggle/working/corpora\"\n    subprocess.run(command.split())\n    nltk.data.path.append('/kaggle/working/')\n\nprint('Imports done.')","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-05-03T11:51:16.968153Z","iopub.execute_input":"2024-05-03T11:51:16.968447Z","iopub.status.idle":"2024-05-03T11:51:26.303520Z","shell.execute_reply.started":"2024-05-03T11:51:16.968418Z","shell.execute_reply":"2024-05-03T11:51:26.302533Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stdout","text":"[nltk_data] Downloading package stopwords to /usr/share/nltk_data...\n[nltk_data]   Package stopwords is already up-to-date!\n[nltk_data] Downloading package punkt to /usr/share/nltk_data...\n[nltk_data]   Package punkt is already up-to-date!\n[nltk_data] Downloading package wordnet to /kaggle/working/...\nArchive:  /kaggle/working/corpora/wordnet.zip\n   creating: /kaggle/working/corpora/wordnet/\n  inflating: /kaggle/working/corpora/wordnet/lexnames  \n  inflating: /kaggle/working/corpora/wordnet/data.verb  \n  inflating: /kaggle/working/corpora/wordnet/index.adv  \n  inflating: /kaggle/working/corpora/wordnet/adv.exc  \n  inflating: /kaggle/working/corpora/wordnet/index.verb  \n  inflating: /kaggle/working/corpora/wordnet/cntlist.rev  \n  inflating: /kaggle/working/corpora/wordnet/data.adj  \n  inflating: /kaggle/working/corpora/wordnet/index.adj  \n  inflating: /kaggle/working/corpora/wordnet/LICENSE  \n  inflating: /kaggle/working/corpora/wordnet/citation.bib  \n  inflating: /kaggle/working/corpora/wordnet/noun.exc  \n  inflating: /kaggle/working/corpora/wordnet/verb.exc  \n  inflating: /kaggle/working/corpora/wordnet/README  \n  inflating: /kaggle/working/corpora/wordnet/index.sense  \n  inflating: /kaggle/working/corpora/wordnet/data.noun  \n  inflating: /kaggle/working/corpora/wordnet/data.adv  \n  inflating: /kaggle/working/corpora/wordnet/index.noun  \n  inflating: /kaggle/working/corpora/wordnet/adj.exc  \nImports done.\n","output_type":"stream"}]},{"cell_type":"code","source":"electronics_dataset = pd.read_csv('../input/amazon-reviews-2018-electronics/labeled_electronics_dataset.csv')\n\nelectronics_dataset.head()","metadata":{"execution":{"iopub.status.busy":"2024-05-03T11:51:26.304672Z","iopub.execute_input":"2024-05-03T11:51:26.305183Z","iopub.status.idle":"2024-05-03T11:51:26.498502Z","shell.execute_reply.started":"2024-05-03T11:51:26.305154Z","shell.execute_reply":"2024-05-03T11:51:26.497444Z"},"trusted":true},"execution_count":3,"outputs":[{"execution_count":3,"output_type":"execute_result","data":{"text/plain":"   overall  vote  reviewTime  \\\n0        2     0  2010-02-10   \n1        2     0  2016-10-24   \n2        1     0  2017-07-10   \n3        4     5  2013-05-02   \n4        3     0  2013-01-04   \n\n                                          reviewText  \\\n0                          Tech support is the worst   \n1  Screws were missing from the bracket and beaut...   \n2  Trouble connecting and staying connected via b...   \n3  I purchased this unit for our RV to replace an...   \n4  It works.  Nuff said but the review requires 1...   \n\n                                             summary     Label  \n0                                         1265760000  NEGATIVE  \n1           Spend a little more and get much better.  NEGATIVE  \n2                                         1499644800  NEGATIVE  \n3  Receiver Offers a Lot of Flexibility & Complexity  POSITIVE  \n4                                       It's a cable   NEUTRAL  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>overall</th>\n      <th>vote</th>\n      <th>reviewTime</th>\n      <th>reviewText</th>\n      <th>summary</th>\n      <th>Label</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>2</td>\n      <td>0</td>\n      <td>2010-02-10</td>\n      <td>Tech support is the worst</td>\n      <td>1265760000</td>\n      <td>NEGATIVE</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>2</td>\n      <td>0</td>\n      <td>2016-10-24</td>\n      <td>Screws were missing from the bracket and beaut...</td>\n      <td>Spend a little more and get much better.</td>\n      <td>NEGATIVE</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>1</td>\n      <td>0</td>\n      <td>2017-07-10</td>\n      <td>Trouble connecting and staying connected via b...</td>\n      <td>1499644800</td>\n      <td>NEGATIVE</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>4</td>\n      <td>5</td>\n      <td>2013-05-02</td>\n      <td>I purchased this unit for our RV to replace an...</td>\n      <td>Receiver Offers a Lot of Flexibility &amp; Complexity</td>\n      <td>POSITIVE</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>3</td>\n      <td>0</td>\n      <td>2013-01-04</td>\n      <td>It works.  Nuff said but the review requires 1...</td>\n      <td>It's a cable</td>\n      <td>NEUTRAL</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"markdown","source":"## Data pre-processing","metadata":{}},{"cell_type":"code","source":"# Check for NaN values\nprint(\"NaN (before cleanup) ?: \\n\", electronics_dataset.isnull().sum())\n\nelectronics_dataset['reviewText'] = electronics_dataset['reviewText'].fillna('')\n\nprint(\"NaN (after cleanup) ?: \\n\", electronics_dataset.isnull().sum())","metadata":{"execution":{"iopub.status.busy":"2024-05-03T11:54:54.288196Z","iopub.execute_input":"2024-05-03T11:54:54.289108Z","iopub.status.idle":"2024-05-03T11:54:54.321383Z","shell.execute_reply.started":"2024-05-03T11:54:54.289073Z","shell.execute_reply":"2024-05-03T11:54:54.320471Z"},"trusted":true},"execution_count":4,"outputs":[{"name":"stdout","text":"NaN (before cleanup) ?: \n overall       0\nvote          0\nreviewTime    0\nreviewText    1\nsummary       0\nLabel         0\ndtype: int64\nNaN (after cleanup) ?: \n overall       0\nvote          0\nreviewTime    0\nreviewText    0\nsummary       0\nLabel         0\ndtype: int64\n","output_type":"stream"}]},{"cell_type":"code","source":"# Text preprocessing for reviewText column\n# Lower all text\n\nelectronics_dataset['reviewText'] = electronics_dataset['reviewText'].str.lower()\n\n# Initialize pandarallel\n# I used pandarallel because it applies the functions much faster than a normal pandas apply.\npandarallel.initialize(nb_workers=4,progress_bar=True)\n\n# Remove all special characters\ndef remove_special_chars(text):\n    return ''.join(x if x.isalnum() else ' ' for x in text)\n\nelectronics_dataset['reviewText'] = electronics_dataset['reviewText'].parallel_apply(remove_special_chars)\n\n# get stopwords.\nstop_words = set(stopwords.words('english'))\n\n# Remove stop_words\ndef remove_stopwords(text):\n    words = word_tokenize(text)\n    return [x for x in words if x not in stop_words]\n\n\nelectronics_dataset['reviewText'] = electronics_dataset['reviewText'].parallel_apply(remove_stopwords)\n\n# Lemmatization\ndef lemmatize_word(text):\n    wordnet = WordNetLemmatizer()\n    return \" \".join([wordnet.lemmatize(word) for word in text])\n\nelectronics_dataset['reviewText'] = electronics_dataset['reviewText'].parallel_apply(lemmatize_word)\n\ndef remove_numbers(text):\n    return re.sub(r'\\d+', '', text)\n\nelectronics_dataset['reviewText'] = electronics_dataset['reviewText'].parallel_apply(remove_numbers)\n\nprint('Example of preprocessing train: ')\nprint(electronics_dataset['reviewText'][0])","metadata":{"execution":{"iopub.status.busy":"2024-05-03T11:54:54.586648Z","iopub.execute_input":"2024-05-03T11:54:54.587028Z","iopub.status.idle":"2024-05-03T11:55:04.567166Z","shell.execute_reply.started":"2024-05-03T11:54:54.586978Z","shell.execute_reply":"2024-05-03T11:55:04.565912Z"},"trusted":true},"execution_count":5,"outputs":[{"name":"stdout","text":"INFO: Pandarallel will run on 4 workers.\nINFO: Pandarallel will use Memory file system to transfer data between the main process and workers.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"VBox(children=(HBox(children=(IntProgress(value=0, description='0.00%', max=4953), Label(value='0 / 4953'))), …","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"617c216e7dda4aef80b651a070abc8c5"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"VBox(children=(HBox(children=(IntProgress(value=0, description='0.00%', max=4953), Label(value='0 / 4953'))), …","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e7985ff33f4545eea0aea84429c2b0cd"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"VBox(children=(HBox(children=(IntProgress(value=0, description='0.00%', max=4953), Label(value='0 / 4953'))), …","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4c02eaa3bd9a4f4cafe133fc8c758f58"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"VBox(children=(HBox(children=(IntProgress(value=0, description='0.00%', max=4953), Label(value='0 / 4953'))), …","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8659e03b55854f7a9d9cfdd60df1982a"}},"metadata":{}},{"name":"stdout","text":"Example of preprocessing train: \ntech support worst\n","output_type":"stream"}]},{"cell_type":"code","source":"X = electronics_dataset['reviewText']\ny = electronics_dataset['Label']\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\n# Initialize LabelEncoder\nlabel_encoder = LabelEncoder()\n\n# Create numerical labels for BERT model\ntrain_numerical_labels = label_encoder.fit_transform(y_train)\ntest_numerical_labels = label_encoder.fit_transform(y_test)\nprint(train_numerical_labels)","metadata":{"execution":{"iopub.status.busy":"2024-05-03T11:55:04.569050Z","iopub.execute_input":"2024-05-03T11:55:04.569362Z","iopub.status.idle":"2024-05-03T11:55:04.591624Z","shell.execute_reply.started":"2024-05-03T11:55:04.569332Z","shell.execute_reply":"2024-05-03T11:55:04.590776Z"},"trusted":true},"execution_count":6,"outputs":[{"name":"stdout","text":"[0 2 0 ... 2 1 0]\n","output_type":"stream"}]},{"cell_type":"markdown","source":"## Distil BERT model creation and evaluation","metadata":{}},{"cell_type":"code","source":"tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased')\nmodel = DistilBertForSequenceClassification.from_pretrained('distilbert-base-uncased', num_labels=3, output_attentions = False, output_hidden_states = False)","metadata":{"execution":{"iopub.status.busy":"2024-05-03T11:55:07.777253Z","iopub.execute_input":"2024-05-03T11:55:07.778190Z","iopub.status.idle":"2024-05-03T11:55:10.824748Z","shell.execute_reply.started":"2024-05-03T11:55:07.778149Z","shell.execute_reply":"2024-05-03T11:55:10.824036Z"},"trusted":true},"execution_count":7,"outputs":[{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/28.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0499c08fa648474fa440ecd0d97f7b6f"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"fdb8bb3a4b1b453ca669c5a21faab07f"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"095c69b62b234d20831152ff8efb6bbc"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/483 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4dcedac9d78947a7b24fc91349333735"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/268M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"78fa59cd5eb24ca8ad4e54c34e544537"}},"metadata":{}},{"name":"stderr","text":"Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"}]},{"cell_type":"code","source":"# Find max lengths of train and test datasets in order to use it later for BERT\nmax_len = 0\n\n# Train dataset\nfor review in X_train:\n\n    # Tokenize the text and add `[CLS]` and `[SEP]` tokens.\n    input_ids = tokenizer.encode(review, add_special_tokens=True)\n\n    # Update the maximum sentence length.\n    max_len = max(max_len, len(input_ids))\n\nprint('Max review length (train): ', max_len)\n\nmax_len = 0\n\n# Test dataset\nfor review in X_test:\n\n    # Tokenize the text and add `[CLS]` and `[SEP]` tokens.\n    input_ids = tokenizer.encode(review, add_special_tokens=True)\n\n    # Update the maximum sentence length.\n    max_len = max(max_len, len(input_ids))\n\nprint('Max review length (test): ', max_len)","metadata":{"execution":{"iopub.status.busy":"2024-05-03T11:55:12.552672Z","iopub.execute_input":"2024-05-03T11:55:12.553053Z","iopub.status.idle":"2024-05-03T11:55:32.367939Z","shell.execute_reply.started":"2024-05-03T11:55:12.553023Z","shell.execute_reply":"2024-05-03T11:55:32.367006Z"},"trusted":true},"execution_count":8,"outputs":[{"name":"stderr","text":"Token indices sequence length is longer than the specified maximum sequence length for this model (636 > 512). Running this sequence through the model will result in indexing errors\n","output_type":"stream"},{"name":"stdout","text":"Max review length (train):  1132\nMax review length (test):  799\n","output_type":"stream"}]},{"cell_type":"code","source":"# ================== TRAIN DATASET ==================\ntrain_input_ids = []\ntrain_attention_masks = []\n\n# Set the seed value to 42 in order to make experiment reproducible.\nseed_val = 42\n\nrandom.seed(seed_val)\nnp.random.seed(seed_val)\ntorch.manual_seed(seed_val)\ntorch.cuda.manual_seed_all(seed_val)\n\n# Train dataset encoding\nfor review in X_train:\n    encoded_dict = tokenizer.encode_plus(\n                        review,                      # review to encode.\n                        add_special_tokens = True, # Add '[CLS]' and '[SEP]'\n                        max_length = 512,           # Pad & truncate all reviews.\n                        padding = 'max_length',\n                        return_attention_mask = True,   # Construct attention masks.\n                        return_tensors = 'pt',     # Return pytorch tensors.\n                        truncation = True # truncate reviews to max length\n                   )\n    \n    # Add the encoded review to the list.    \n    train_input_ids.append(encoded_dict['input_ids'])\n    \n    # And its attention mask (simply differentiates padding from non-padding).\n    train_attention_masks.append(encoded_dict['attention_mask'])\n\n# Move all tensors and model to GPU.\ndevice = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n# device = torch.device(\"cuda:1\")\n\n# Make lists into tensors\ntrain_input_ids = torch.cat(train_input_ids, dim=0).to(device)\ntrain_attention_masks = torch.cat(train_attention_masks, dim=0).to(device)\ntrain_labels = torch.tensor(train_numerical_labels).to(device)\n\n\n\ntrain_input_ids = train_input_ids.to(device)\ntrain_attention_masks = train_attention_masks.to(device)\ntrain_labels = train_labels.to(device)\n\ntrain_dataset = torch.utils.data.TensorDataset(train_input_ids, train_attention_masks, train_labels)\ntrain_dataloader = torch.utils.data.DataLoader(train_dataset, batch_size=16, shuffle=True)\n\n# ================== TEST DATASET ==================\n# Tokenize test data\ntest_input_ids = []\ntest_attention_masks = []\n\n# Valid dataset encoding\nfor review in X_test:\n    encoded_dict = tokenizer.encode_plus(\n                        review,                      # review to encode.\n                        add_special_tokens = True, # Add '[CLS]' and '[SEP]'\n                        max_length = 512,           # Pad & truncate all reviews.\n                        padding = 'max_length',\n                        return_attention_mask = True,   # Construct attention masks.\n                        return_tensors = 'pt',     # Return pytorch tensors.\n                        truncation = True # truncate reviews to max length\n                   )\n    \n    # Add the encoded review to the list.    \n    test_input_ids.append(encoded_dict['input_ids'])\n    \n    # And its attention mask (simply differentiates padding from non-padding).\n    test_attention_masks.append(encoded_dict['attention_mask'])\n\n\n# Make lists into tensors and move them to GPU\ntest_input_ids = torch.cat(test_input_ids, dim=0).to(device)\ntest_attention_masks = torch.cat(test_attention_masks, dim=0).to(device)\n\n# Create DataLoader for test data\ntest_dataset = torch.utils.data.TensorDataset(test_input_ids, test_attention_masks)\ntest_dataloader = torch.utils.data.DataLoader(test_dataset, batch_size=16)\n\nprint(\"Data loading complete.\")","metadata":{"execution":{"iopub.status.busy":"2024-05-03T11:55:33.706390Z","iopub.execute_input":"2024-05-03T11:55:33.706760Z","iopub.status.idle":"2024-05-03T11:56:01.945558Z","shell.execute_reply.started":"2024-05-03T11:55:33.706732Z","shell.execute_reply":"2024-05-03T11:56:01.944575Z"},"trusted":true},"execution_count":9,"outputs":[{"name":"stdout","text":"Data loading complete.\n","output_type":"stream"}]},{"cell_type":"code","source":"learning_rate = 2e-5\nnum_epochs = 3\noptimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n\nmodel.to(device)\ntraining_f1_scores = []\nval_f1_scores = []\n\nfor epoch in range(num_epochs):\n    model.train()\n    total_loss = 0\n    train_predictions = []\n    train_true_labels = []\n    start_time = time.time()\n    for batch in train_dataloader:\n        optimizer.zero_grad()\n        input_ids, attention_mask, labels = batch\n        outputs = model(input_ids, attention_mask=attention_mask, labels=labels)\n        loss = outputs.loss\n        logits = outputs.logits\n        probabilities = torch.softmax(logits, dim=1)\n        predicted_labels = torch.argmax(probabilities, dim=1)\n        train_predictions.extend(predicted_labels.tolist())\n        train_true_labels.extend(labels.tolist())\n        total_loss += loss.item()\n        loss.backward()\n        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0) # Added gradient clipping as well\n        optimizer.step()\n    \n    train_f1 = f1_score(train_true_labels, train_predictions, average='macro')\n    training_f1_scores.append(train_f1)\n    end_time = time.time()\n    elapsed_time = end_time - start_time\n    print(f\"Epoch {epoch+1}, Average Loss: {total_loss / len(train_dataloader)}\")\n    print(\"Elapsed time:\", elapsed_time, \"seconds\")\n    print(\"\\n\")","metadata":{"execution":{"iopub.status.busy":"2024-05-03T11:56:01.947500Z","iopub.execute_input":"2024-05-03T11:56:01.947928Z","iopub.status.idle":"2024-05-03T12:31:06.856518Z","shell.execute_reply.started":"2024-05-03T11:56:01.947892Z","shell.execute_reply":"2024-05-03T12:31:06.855474Z"},"trusted":true},"execution_count":10,"outputs":[{"name":"stdout","text":"Epoch 1, Average Loss: 0.8043767292311886\nElapsed time: 698.7799525260925 seconds\n\n\nEpoch 2, Average Loss: 0.6621243800031911\nElapsed time: 702.8214478492737 seconds\n\n\nEpoch 3, Average Loss: 0.5321063803420538\nElapsed time: 703.215940952301 seconds\n\n\n","output_type":"stream"}]},{"cell_type":"code","source":"# Test predictions\ntest_predictions = []\n\n# Iterate over test set batches\nfor batch in test_dataloader:\n    batch = tuple(t.to(device) for t in batch)\n    input_ids, attention_mask = batch\n    \n    # Disable gradient calculation\n    with torch.no_grad():\n        outputs = model(input_ids, attention_mask=attention_mask)\n    \n    logits = outputs.logits\n    probabilities = torch.softmax(logits, dim=1)\n    predictions = torch.argmax(probabilities, dim=1)\n    \n    # Convert predictions to CPU and append to test_predictions list\n    test_predictions.extend(predictions.cpu().numpy())\n\n# print(\"Test set predictions:\", test_predictions)\nprint(\"Test prediction done.\")","metadata":{"execution":{"iopub.status.busy":"2024-05-03T12:31:06.858273Z","iopub.execute_input":"2024-05-03T12:31:06.858601Z","iopub.status.idle":"2024-05-03T12:32:14.349026Z","shell.execute_reply.started":"2024-05-03T12:31:06.858574Z","shell.execute_reply":"2024-05-03T12:32:14.348043Z"},"trusted":true},"execution_count":11,"outputs":[{"name":"stdout","text":"Test prediction done.\n","output_type":"stream"}]},{"cell_type":"code","source":"string_pred_preds = label_encoder.inverse_transform(test_predictions)\nprint(classification_report(y_test,string_pred_preds))\n\nprint(\"accuracy: \",accuracy_score(y_test,string_pred_preds))\nprint(\"f1: \",f1_score(y_test,string_pred_preds,average='micro'))\nprint(\"total f1: \",f1_score(y_test,string_pred_preds,average=None))","metadata":{"execution":{"iopub.status.busy":"2024-05-03T12:32:14.350835Z","iopub.execute_input":"2024-05-03T12:32:14.351695Z","iopub.status.idle":"2024-05-03T12:32:14.621136Z","shell.execute_reply.started":"2024-05-03T12:32:14.351657Z","shell.execute_reply":"2024-05-03T12:32:14.620153Z"},"trusted":true},"execution_count":12,"outputs":[{"name":"stdout","text":"              precision    recall  f1-score   support\n\n    NEGATIVE       0.65      0.83      0.73      1575\n     NEUTRAL       0.43      0.32      0.36       808\n    POSITIVE       0.79      0.69      0.73      1579\n\n    accuracy                           0.67      3962\n   macro avg       0.62      0.61      0.61      3962\nweighted avg       0.66      0.67      0.66      3962\n\naccuracy:  0.6665825340737002\nf1:  0.6665825340737002\ntotal f1:  [0.73002523 0.36467236 0.73349136]\n","output_type":"stream"}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}